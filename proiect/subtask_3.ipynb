{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a310c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7a310c2",
        "outputId": "93c819ba-c09d-43c3-db52-71a5abb1104b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import json, os, torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5777ad6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "31f39986",
      "metadata": {
        "id": "31f39986"
      },
      "outputs": [],
      "source": [
        "def build_category_list():\n",
        "    # Restaurant\n",
        "    restaurant_entities = [\n",
        "        \"RESTAURANT\", \"FOOD\", \"DRINKS\", \"AMBIENCE\", \"SERVICE\", \"LOCATION\"\n",
        "    ]\n",
        "    restaurant_attrs = [\n",
        "        \"GENERAL\", \"PRICES\", \"QUALITY\", \"STYLE_OPTIONS\", \"MISCELLANEOUS\"\n",
        "    ]\n",
        "\n",
        "    # Laptop\n",
        "    laptop_entities = [\n",
        "        \"LAPTOP\", \"DISPLAY\", \"KEYBOARD\", \"MOUSE\", \"MOTHERBOARD\", \"CPU\",\n",
        "        \"FANS_COOLING\", \"PORTS\", \"MEMORY\", \"POWER_SUPPLY\", \"OPTICAL_DRIVES\",\n",
        "        \"BATTERY\", \"GRAPHICS\", \"HARD_DISK\", \"MULTIMEDIA_DEVICES\", \"HARDWARE\",\n",
        "        \"SOFTWARE\", \"OS\", \"WARRANTY\", \"SHIPPING\", \"SUPPORT\", \"COMPANY\"\n",
        "    ]\n",
        "    laptop_attrs = [\n",
        "        \"GENERAL\", \"PRICE\", \"QUALITY\", \"DESIGN_FEATURES\",\n",
        "        \"OPERATION_PERFORMANCE\", \"USABILITY\", \"PORTABILITY\",\n",
        "        \"CONNECTIVITY\", \"MISCELLANEOUS\"\n",
        "    ]\n",
        "\n",
        "    # Hotel\n",
        "    hotel_entities = [\n",
        "        \"HOTEL\", \"ROOMS\", \"FACILITIES\", \"ROOM_AMENITIES\",\n",
        "        \"SERVICE\", \"LOCATION\", \"FOOD_DRINKS\"\n",
        "    ]\n",
        "    hotel_attrs = [\n",
        "        \"GENERAL\", \"PRICE\", \"COMFORT\", \"CLEANLINESS\",\n",
        "        \"QUALITY\", \"DESIGN_FEATURES\",\n",
        "        \"STYLE_OPTIONS\", \"MISCELLANEOUS\"\n",
        "    ]\n",
        "\n",
        "    # OUT_OF_SCOPE\n",
        "    out_of_scope_entities = [\"OUT_OF_SCOPE\"]\n",
        "    out_of_scope_attrs = [\n",
        "        \"GENERAL\",\n",
        "        \"OPERATION_PERFORMANCE\",\n",
        "        \"DESIGN_FEATURES\",\n",
        "        \"USABILITY\"\n",
        "    ]\n",
        "\n",
        "    categories = []\n",
        "\n",
        "    # Restaurant\n",
        "    for e in restaurant_entities:\n",
        "        for a in restaurant_attrs:\n",
        "            categories.append(f\"{e}#{a}\")\n",
        "\n",
        "    # Laptop\n",
        "    for e in laptop_entities:\n",
        "        for a in laptop_attrs:\n",
        "            categories.append(f\"{e}#{a}\")\n",
        "\n",
        "    # Hotel\n",
        "    for e in hotel_entities:\n",
        "        for a in hotel_attrs:\n",
        "            categories.append(f\"{e}#{a}\")\n",
        "\n",
        "    # OUT_OF_SCOPE\n",
        "    for e in out_of_scope_entities:\n",
        "        for a in out_of_scope_attrs:\n",
        "            categories.append(f\"{e}#{a}\")\n",
        "\n",
        "    return categories\n",
        "\n",
        "\n",
        "# Label mappings\n",
        "ALL_CATEGORIES = build_category_list()\n",
        "category2id = {cat: i for i, cat in enumerate(ALL_CATEGORIES)}\n",
        "id2category = {i: cat for cat, i in category2id.items()}\n",
        "\n",
        "NUM_LABELS = len(ALL_CATEGORIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3b01fd2d",
      "metadata": {
        "id": "3b01fd2d"
      },
      "outputs": [],
      "source": [
        "def build_input(text, aspect, opinion, va):\n",
        "    return (\n",
        "        f\"Text: {text} \"\n",
        "        f\"Aspect: {aspect}. \"\n",
        "        f\"Opinion: {opinion}. \"\n",
        "        f\"VA: {va}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bfd0fe94",
      "metadata": {
        "id": "bfd0fe94"
      },
      "outputs": [],
      "source": [
        "# DATASET PyTorch\n",
        "\n",
        "class CategoryDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, category2id, max_len=128):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                text = data[\"Text\"]\n",
        "\n",
        "                for t in data[\"Quadruplet\"]:\n",
        "                    input_text = build_input(\n",
        "                        text,\n",
        "                        t[\"Aspect\"],\n",
        "                        t[\"Opinion\"],\n",
        "                        t[\"VA\"]\n",
        "                    )\n",
        "\n",
        "                    label = category2id[t[\"Category\"]]\n",
        "\n",
        "                    self.samples.append((input_text, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "42f07241",
      "metadata": {
        "id": "42f07241"
      },
      "outputs": [],
      "source": [
        "# MODEL BERT + CLS CLASSIFICATION\n",
        "\n",
        "def build_model(num_labels):\n",
        "    return BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=num_labels\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a74baa98",
      "metadata": {
        "id": "a74baa98"
      },
      "outputs": [],
      "source": [
        "def train_model(model,\n",
        "                dataloader,\n",
        "                optimizer,\n",
        "                device,\n",
        "                epochs=10,\n",
        "                patience=3,\n",
        "                min_delta=1e-4):\n",
        "    best_macro_f1 = 0.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch+1}] \"\n",
        "            f\"Loss: {avg_loss:.4f} | \"\n",
        "            f\"Acc: {acc:.4f} | \"\n",
        "            f\"Macro-F1: {macro_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        # ---- EARLY STOPPING ----\n",
        "        if macro_f1 > best_macro_f1 + min_delta:\n",
        "            best_macro_f1 = macro_f1\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(\n",
        "                f\"  - No improvement for {epochs_no_improve}/{patience} epochs\"\n",
        "            )\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\n",
        "                f\"!! Early stopping triggered at epoch {epoch+1}. \"\n",
        "                f\"Best Macro-F1: {best_macro_f1:.4f}\"\n",
        "            )\n",
        "            break\n",
        "\n",
        "    return best_macro_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f650ce49",
      "metadata": {
        "id": "f650ce49"
      },
      "outputs": [],
      "source": [
        "def predict_category(model, tokenizer, device, text, aspect, opinion, va):\n",
        "    model.eval()\n",
        "\n",
        "    input_text = build_input(text, aspect, opinion, va)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "\n",
        "    pred_id = torch.argmax(logits, dim=1).item()\n",
        "    return id2category[pred_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "iPlIyNWyk4fh",
      "metadata": {
        "id": "iPlIyNWyk4fh"
      },
      "outputs": [],
      "source": [
        "def evaluate_saved_model(\n",
        "    save_dir,\n",
        "    data_path,\n",
        "    category2id,\n",
        "    device,\n",
        "    batch_size=8\n",
        "):\n",
        "\n",
        "    # Load model & tokenizer\n",
        "    model, tokenizer = load_model(save_dir, device)\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    dataset = CategoryDataset(\n",
        "        path=data_path,\n",
        "        tokenizer=tokenizer,\n",
        "        category2id=category2id\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    print(\"\\n EVALUATE ON TEST DATA\")\n",
        "    print(f\"Loss:      {avg_loss:.4f}\")\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Macro-F1:  {macro_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": macro_f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "11936862",
      "metadata": {
        "id": "11936862"
      },
      "outputs": [],
      "source": [
        "def save_model(model, tokenizer, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6a54741f",
      "metadata": {
        "id": "6a54741f"
      },
      "outputs": [],
      "source": [
        "def load_model(save_dir, device):\n",
        "    tokenizer = BertTokenizer.from_pretrained(save_dir)\n",
        "    model = BertForSequenceClassification.from_pretrained(save_dir)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ea20ca19",
      "metadata": {
        "id": "ea20ca19"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/nlp_proiect/modele/25_epoci\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    file_name = \"/content/drive/MyDrive/nlp_proiect/nlp_data/eng_laptop_train_alltasks.jsonl\"\n",
        "\n",
        "    train_dataset = CategoryDataset(\n",
        "        path=file_name,\n",
        "        tokenizer=tokenizer,\n",
        "        category2id=category2id\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    model = build_model(NUM_LABELS)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=2e-5,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    train_model(\n",
        "        model=model,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        epochs=30,\n",
        "        patience=3\n",
        "    )\n",
        "\n",
        "    save_model(model, tokenizer, SAVE_DIR)\n",
        "\n",
        "    evaluate_saved_model(\n",
        "      save_dir=SAVE_DIR,\n",
        "      data_path=\"/content/drive/MyDrive/nlp_proiect/nlp_data/eng_laptop_train_alltasks.jsonl\",\n",
        "      category2id=category2id,\n",
        "      device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3f96a8a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f96a8a7",
        "outputId": "2126cbc8-00c2-4849-bf0c-238955fdfd89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Loss: 3.2856 | Acc: 0.3113 | Macro-F1: 0.0247\n",
            "[Epoch 2] Loss: 2.0165 | Acc: 0.5117 | Macro-F1: 0.0729\n",
            "[Epoch 3] Loss: 1.5237 | Acc: 0.6139 | Macro-F1: 0.1152\n",
            "[Epoch 4] Loss: 1.2098 | Acc: 0.6858 | Macro-F1: 0.1557\n",
            "[Epoch 5] Loss: 0.9784 | Acc: 0.7362 | Macro-F1: 0.1951\n",
            "[Epoch 6] Loss: 0.7886 | Acc: 0.7911 | Macro-F1: 0.2530\n",
            "[Epoch 7] Loss: 0.6398 | Acc: 0.8320 | Macro-F1: 0.3016\n",
            "[Epoch 8] Loss: 0.5201 | Acc: 0.8670 | Macro-F1: 0.3455\n",
            "[Epoch 9] Loss: 0.4154 | Acc: 0.8992 | Macro-F1: 0.4174\n",
            "[Epoch 10] Loss: 0.3494 | Acc: 0.9130 | Macro-F1: 0.4522\n",
            "[Epoch 11] Loss: 0.2989 | Acc: 0.9293 | Macro-F1: 0.5093\n",
            "[Epoch 12] Loss: 0.2444 | Acc: 0.9456 | Macro-F1: 0.5557\n",
            "[Epoch 13] Loss: 0.1891 | Acc: 0.9595 | Macro-F1: 0.6249\n",
            "[Epoch 14] Loss: 0.1766 | Acc: 0.9598 | Macro-F1: 0.6367\n",
            "[Epoch 15] Loss: 0.1555 | Acc: 0.9667 | Macro-F1: 0.6767\n",
            "[Epoch 16] Loss: 0.1429 | Acc: 0.9692 | Macro-F1: 0.7111\n",
            "[Epoch 17] Loss: 0.1066 | Acc: 0.9777 | Macro-F1: 0.7248\n",
            "[Epoch 18] Loss: 0.1191 | Acc: 0.9709 | Macro-F1: 0.7666\n",
            "[Epoch 19] Loss: 0.1073 | Acc: 0.9759 | Macro-F1: 0.8199\n",
            "[Epoch 20] Loss: 0.0865 | Acc: 0.9782 | Macro-F1: 0.8390\n",
            "[Epoch 21] Loss: 0.0688 | Acc: 0.9841 | Macro-F1: 0.8875\n",
            "[Epoch 22] Loss: 0.0653 | Acc: 0.9835 | Macro-F1: 0.8970\n",
            "[Epoch 23] Loss: 0.0880 | Acc: 0.9778 | Macro-F1: 0.9075\n",
            "[Epoch 24] Loss: 0.0809 | Acc: 0.9785 | Macro-F1: 0.9073\n",
            "  - No improvement for 1/3 epochs\n",
            "[Epoch 25] Loss: 0.0573 | Acc: 0.9875 | Macro-F1: 0.9641\n",
            "[Epoch 26] Loss: 0.0422 | Acc: 0.9896 | Macro-F1: 0.9715\n",
            "[Epoch 27] Loss: 0.0533 | Acc: 0.9856 | Macro-F1: 0.9573\n",
            "  - No improvement for 1/3 epochs\n",
            "[Epoch 28] Loss: 0.0503 | Acc: 0.9865 | Macro-F1: 0.9442\n",
            "  - No improvement for 2/3 epochs\n",
            "[Epoch 29] Loss: 0.0632 | Acc: 0.9825 | Macro-F1: 0.9606\n",
            "  - No improvement for 3/3 epochs\n",
            "!! Early stopping triggered at epoch 29. Best Macro-F1: 0.9715\n",
            "\n",
            " EVALUATE ON TEST DATA\n",
            "Loss:      0.0434\n",
            "Accuracy:  0.9880\n",
            "Macro-F1:  0.9934\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85dcb680",
      "metadata": {},
      "outputs": [],
      "source": [
        "### PT INTEGRARE IN PROIECT\n",
        "\n",
        "\n",
        "SAVE_DIR = \"task_3_saved_model\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model, tokenizer = load_model(SAVE_DIR, device)\n",
        "\n",
        "text = \"the trackpad works well and the screen display is great too .\"\n",
        "aspect = \"trackpad\"\n",
        "opinion = \"well\"\n",
        "va = \"6.50#6.62\"\n",
        "\n",
        "category = predict_category(model, tokenizer, device, text, aspect, opinion, va)\n",
        "\n",
        "print(f\"Predicted category: {category}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
